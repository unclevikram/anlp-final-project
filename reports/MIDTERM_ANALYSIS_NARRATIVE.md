Argument Structure in Online Debates: Midterm Analysis and Feasibility

This project investigates how argumentative structure varies with stance in online debates and adjacent genres. Rather than focusing exclusively on stance prediction or sentiment, we examine the internal organization of arguments—how claims and premises are arranged, which relations (support vs. attack) are employed, and whether the resulting graphs are broader (multiple parallel supports) or deeper (multi‑step chains of reasoning). Our central hypothesis is that pro and con positions on the same topic adopt measurably different structural strategies. Pro arguments are expected to construct positive cases with denser evidence and broader support, whereas con arguments are expected to devote more effort to refutation, reflected in a higher proportion of attack relations. A secondary question concerns feasibility and transfer: to what extent can models trained on well‑annotated essay data support proxy‑level analyses in less richly annotated debate corpora?

Data and Scope. We organize our analysis around four corpora that jointly span richly annotated essays, topic‑keypoint pairings, standalone arguments with quality labels, and conversational threads. First, the Persuasive Essays corpus (Stab & Gurevych, 2017) provides gold annotations for argumentative components (MajorClaim, Claim, Premise) and relations (Support, Attack). This offers a controlled setting for component span detection, relation classification, and reliable graph‑level measurements (breadth, depth, density, evidence per claim, attack ratio). Second, IBM ArgKP‑2021 contains argument–key point pairs with topic and stance as well as binary match labels. Although it lacks gold component spans and explicit support/attack links, the match label can be interpreted as a support proxy (an argument instantiates or supports a key point), enabling stance‑partitioned breadth and density proxies by topic. Third, IBM Argument Quality (30k) provides stance and continuous quality labels for individual arguments; with an essay‑trained component detector, we can derive structure proxies (e.g., premises‑per‑claim ratio) and relate them to quality. Finally, the ChangeMyView (CMV) conversational dataset offers interaction structure without gold argument annotations; we treat it as an exploratory resource for proxy‑based comparisons (argumentative sentence detection by transfer; reply‑graph directionality as a crude support/attack cue) rather than a primary evaluation set.

Methods and Infrastructure. We implemented a reproducible pipeline for the essay corpus that parses BRAT annotations, constructs argument graphs, and computes structural metrics. A key design decision is to respect edge semantics in BRAT: support links run from premises (and sub‑premises) into claims. We therefore compute breadth as the number of incoming support edges per claim (averaged across claims) and depth as the longest chain of incoming support ending at a claim or major claim. This corrected an early artifact where breadth appeared compressed when support edges were treated as outgoing from claims. For stance, we developed a lightweight auto‑labeling procedure that combines explicit textual cues (agree/disagree; should/should not) with promptspecific heuristics (positive vs. negative trend, allow vs. ban, good vs. bad idea, advantages vs. disadvantages), applied to both the essay body and the claim/major‑claim texts. To preserve precision, essays without clear signals remain unlabeled and can be resolved quickly by manual inspection.

For ArgKP, we implemented a minimal, transparent baseline that treats argument–key point matching as a lexical similarity problem (Jaccard over token sets with tuned threshold). This baseline is not intended to be competitive; rather, it establishes feasibility and headroom for more capable cross‑encoders (e.g., RoBERTa) that model semantic alignment between arguments and key points. The ArgKP analyzer also computes stance‑partitioned topic summaries (e.g., arguments per key point and argument density per topic) to support broader rhetorical comparisons across sides.

Preliminary Findings. Using conservative auto‑labels, preliminary stance‑wise comparisons in the essay corpus already align with our hypotheses. Pro essays show higher evidence density (more premises per claim) and greater breadth (more supporters per claim), while con essays show a higher attack ratio. Depth differences are small, consistent with the relatively shallow, single‑step support chains common in essays. These effects emerge even before full stance coverage and without any learned models, indicating that the signal is not trivially driven by modeling artifacts. On ArgKP, the lexical baseline achieves test F1 in the mid‑0.3s (high recall, low precision), which is expected given the simplicity of the method and the subtlety of matching at the key‑point level. This validates problem framing and underscores the likely gains from cross‑encoders that can capture paraphrase and entailment beyond surface overlap.

Unexpected Results and Corrections. Early plots suggested minimal differences in breadth between stances, which contradicted both theoretical expectations and qualitative reading. The cause was methodological: breadth was initially computed using outgoing edges at claims, whereas BRAT encodes supports as edges directed into claims from their supporting premises. After redefining breadth as the mean number of incoming supports per claim and depth as the length of incoming support chains, stance differences became visible and interpretable. This emphasizes the importance of aligning graph computations with annotation semantics. A second source of uncertainty is the sparsity of attack edges in essays; to address this, we report effect sizes and confidence intervals, and we pair topics across stances to control for topical confounds.

Evaluation Strategy. For essays, we will report span‑level precision/recall/F1 for component tagging, macro‑F1 for relation classification, and stance‑wise structural statistics with paired tests and Cohen’s d to quantify differences within topics. For ArgKP, we will report matching performance (F1/AUC) and analyze stance‑partitioned proxies (e.g., breadth as arguments per key point; density as arguments per topic). For Argument Quality, we will correlate quality with structure proxies derived from an essay‑trained component detector (Spearman’s ρ with bootstrap confidence intervals and partial correlations controlling for length). Exploratory CMV analyses will present proxy distributions and small manual validations to bound noise. Across datasets, we will include compact tables per corpus and a small set of figures (e.g., attack ratio and evidence density by stance in essays; breadth proxies by stance in ArgKP) to foreground the most theoretically relevant effects.

Planned Improvements. The current pipeline demonstrates feasibility and surfaces meaningful stance effects using only gold annotations and simple heuristics. To strengthen results and broaden coverage, we will: (i) train a transformer‑based token classifier for component spans in essays; (ii) train a pairwise transformer for relation classification (support/attack/none) with light constraints to improve graph coherence; and (iii) deploy an argument–key point cross‑encoder on ArgKP to replace lexical overlap with semantic matching. These models will enable more robust graph reconstruction in essays (for ablation and replication) and provide higher‑quality structure proxies in ArgKP and CMV. We will also finalize stance labeling in the essays (brief manual pass over unlabeled rows) and extend our statistical analysis with bootstrap confidence intervals and permutation tests for stance differences.

Limitations and Mitigations. Differences in annotation granularity across datasets necessitate proxy definitions outside the essay corpus. We address this by mapping all corpora to a common schema (nodes: claim/premise/key point; edges: support/attack/match) and by clearly labeling proxies where gold labels are unavailable. Attack edges are infrequent in essays, which can reduce power; pairing topics and reporting effect sizes mitigates this risk. Auto‑label heuristics may miss stance in implicit essays; we deliberately favor precision and reserve a brief manual pass for coverage. Finally, transfer to conversational domains introduces additional noise (ellipsis, pragmatics); we will treat CMV as exploratory and avoid over‑interpretation.

Outlook. Taken together, the preliminary evidence supports our central claim: stance is reflected in argument structure. Pro positions tend toward denser, broader support of claims; con positions devote relatively more structure to refutation. By moving from heuristic baselines to modern transformer models and by completing stance coverage, we expect improvements in both predictive accuracy and the stability of structural measurements. The resulting analysis will be of interest to computational argumentation researchers (linking stance to structure), educators and debate coaches (evidence‑based guidance on rhetorical strategies), and NLP practitioners (methods for harvesting and comparing argumentative structures across domains). The methodological contributions—transparent graph computations aligned with annotation semantics, stance‑aware structural metrics, and proxy mapping across datasets—provide a reusable foundation for future work on argument quality, persuasion, and domain transfer.



